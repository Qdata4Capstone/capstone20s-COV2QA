{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_proj_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU7RHRg2akyn"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMe1goA3MSKb"
      },
      "source": [
        "#CONSTANTS#\n",
        "PATH = '/content/drive/My Drive/mrna_data/'\n",
        "KNN_CANDIDATES = [1,2,3,8,10,13,15, 20, 25,27,28]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNypiYmYpBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b127ad-6777-487e-9b35-9b1b91bdc843"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7n2hoG3bNen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "d002d85e-3631-46a2-a3d2-770f4d7af08b"
      },
      "source": [
        "#PREPROCESSING#\n",
        "# define the triplet bases\n",
        "def define_lang():\n",
        "  lang = []\n",
        "  amino = \"UACG\"\n",
        "  for c1 in amino:\n",
        "    for c2 in amino:\n",
        "      for c3 in amino:\n",
        "        lang.append(c1+c2+c3)\n",
        "  lang = np.array(lang)\n",
        "\n",
        "  return lang\n",
        "#num_clusters = 1000\n",
        "\n",
        "# convert each sequence of bases to a frequency count of the triplets\n",
        "def seq_to_freq(seq):\n",
        "  freq = []\n",
        "  lang = define_lang()\n",
        "  for word in lang:\n",
        "    #print(seq.split(word))\n",
        "    freq.append(len(seq.split(word))-1)\n",
        "  freq = np.array(freq)\n",
        "  return freq\n",
        "\n",
        "\"\"\"def make_clusters(train):\n",
        "  kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(train)\n",
        "  return kmeans.labels_\n",
        "\n",
        "def avg_clusters(labels, x_train, y_train):\n",
        "  new_x_train = [[0.0]*64] * num_clusters\n",
        "  new_x_train = np.array(new_x_train)\n",
        "  new_y_train = [[0.0]*len(y_train[0])] * num_clusters\n",
        "  new_y_train = np.array(new_y_train)\n",
        "\n",
        "  for i in range(len(x_train)):\n",
        "    new_x_train[labels[i]] = np.add(new_x_train[labels[i]], x_train[i])\n",
        "  num_molec_per_cluster = len(x_train)/num_clusters\n",
        "  for i in range(len(new_x_train)):\n",
        "    new_x_train[i] = new_x_train[i]/num_molec_per_cluster\n",
        "\n",
        "  for i in range(len(y_train)):\n",
        "    new_y_train[labels[i]] = np.add(new_y_train[labels[i]], y_train[i])\n",
        "  for i in range(len(new_y_train)):\n",
        "    new_y_train[i] = new_y_train[i]/num_molec_per_cluster\n",
        "  \n",
        "  return new_x_train, new_y_train\"\"\"\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def make_clusters(train):\\n  kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(train)\\n  return kmeans.labels_\\n\\ndef avg_clusters(labels, x_train, y_train):\\n  new_x_train = [[0.0]*64] * num_clusters\\n  new_x_train = np.array(new_x_train)\\n  new_y_train = [[0.0]*len(y_train[0])] * num_clusters\\n  new_y_train = np.array(new_y_train)\\n\\n  for i in range(len(x_train)):\\n    new_x_train[labels[i]] = np.add(new_x_train[labels[i]], x_train[i])\\n  num_molec_per_cluster = len(x_train)/num_clusters\\n  for i in range(len(new_x_train)):\\n    new_x_train[i] = new_x_train[i]/num_molec_per_cluster\\n\\n  for i in range(len(y_train)):\\n    new_y_train[labels[i]] = np.add(new_y_train[labels[i]], y_train[i])\\n  for i in range(len(new_y_train)):\\n    new_y_train[i] = new_y_train[i]/num_molec_per_cluster\\n  \\n  return new_x_train, new_y_train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8KYFuQybYSI"
      },
      "source": [
        "#KNN#\n",
        "\n",
        "# calculate the distance between two frequency vectors\n",
        "def dist(freq1, freq2):\n",
        "  dist = np.linalg.norm(freq1-freq2)\n",
        "  return dist\n",
        "\n",
        "# find the kNN for a single frequency vector\n",
        "def find_knn(x_test_vector, x_train, knn):\n",
        "  # dictionary containing keys as distances from x_test_vector to every vector in x_train,\n",
        "  # and values as the indices of the vectors in x_train that correspond to these distances\n",
        "  distances = {}\n",
        "\n",
        "  # find distance between x_test_vector and every vector in x_train\n",
        "  for vec_idx in range(len(x_train)):\n",
        "    d = dist(x_test_vector, x_train[vec_idx])\n",
        "\n",
        "    # record distance and index in the dictionary \n",
        "    if d in distances:\n",
        "      distances[d].append(vec_idx)\n",
        "    else:\n",
        "      distances[d] = [vec_idx]\n",
        "\n",
        "  # sort the distances from least to greatest\n",
        "  sorted_distances = list(distances.keys())\n",
        "  sorted_distances.sort()\n",
        "\n",
        "  # get the kNN for x_test_vector\n",
        "  knn_arr = []\n",
        "\n",
        "  # record indices corresponding to first knn distances \n",
        "  for i in range(knn):\n",
        "    d = sorted_distances[i]\n",
        "    for idx in distances[d]:\n",
        "      knn_arr.append(idx)\n",
        "  # truncate knn in case multiple indices correspond to one distance\n",
        "  knn_arr = knn_arr[:knn]\n",
        "\n",
        "  return knn_arr \n",
        "\n",
        "def predict(x_test, x_train, y_train, knn):\n",
        "  '''\n",
        "  parameters:\n",
        "    x_test: array of frequency vectors to be predicted\n",
        "    x_train: array of frequency vectors from which neighbors will be found\n",
        "    y_train: array of reactivity values corresponding to each frequency vector\n",
        "    knn: number of neighbors to find\n",
        "  '''\n",
        "  y_pred = []\n",
        "\n",
        "  # predict reactivity values for each vector in x_test \n",
        "  for vector in x_test:\n",
        "    knn_arr = find_knn(vector, x_train, knn)\n",
        "\n",
        "    # calculate prediction\n",
        "    y_pred_vector = np.zeros((len(y_train[0]), 3))\n",
        "    # sum the reactivities in each neighbor\n",
        "    for idx in knn_arr:\n",
        "      y_pred_vector = np.add(y_pred_vector, y_train[idx])\n",
        "    # divide each sum by knn to get the average reactivity\n",
        "    y_pred_vector = np.divide(y_pred_vector, knn)\n",
        "    \n",
        "    # store this prediction in the prediction vector\n",
        "    y_pred.append(y_pred_vector)\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "# calculate error\n",
        "def error(y_pred, y_test, metric='MCRMSE'):\n",
        "  err = 0\n",
        "  if metric == 'MSE':\n",
        "    mse = 0\n",
        "\n",
        "    # sum the squared differences of each prediction\n",
        "    for vector_idx in range(len(y_pred)):\n",
        "      # for each reactivity vector, find the squared differences of each prediction\n",
        "      diff_square_vector = np.subtract(y_test[vector_idx], y_pred[vector_idx])\n",
        "      diff_square_vector = np.power(diff_square_vector, 2)\n",
        "\n",
        "      # sum the values of these squared differences and add to mse\n",
        "      mse += np.sum(diff_square_vector)\n",
        "\n",
        "    # divide by total number of predictions\n",
        "    mse /= (len(y_pred) * len(y_pred[0]))\n",
        "    err = mse\n",
        "  if metric == \"MCRMSE\":\n",
        "    #mean columnwise root mean squared error)#\n",
        "    #error specified by kaggle competition, accounts for all five categories being measured#\n",
        "    mcrmse = 0\n",
        "\n",
        "    y_pred_t = np.transpose(np.array(y_pred), (2, 0, 1))\n",
        "    y_test_t = np.transpose(y_test, (2, 0, 1))\n",
        "    for i in range(3):\n",
        "      mse = error(y_pred_t[i], y_test_t[i], 'MSE')\n",
        "      mcrmse += mse**0.5\n",
        "\n",
        "    # divide by total number of prediction groups\n",
        "    err = mcrmse / 3\n",
        "\n",
        "\n",
        "  return err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY5vCyfnRzpc"
      },
      "source": [
        "def find_best_k(x_train, x_test, y_train, y_test):\n",
        "  best_k = 0\n",
        "  best_err = 100\n",
        "\n",
        "  for k in KNN_CANDIDATES:\n",
        "    y_pred = predict(x_test, x_train, y_train, k)\n",
        "    err = error(y_pred, y_test)\n",
        "\n",
        "    if err < best_err:\n",
        "      best_k = k\n",
        "      best_err = err\n",
        "\n",
        "    print(k, err)\n",
        "\n",
        "  return best_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjFt2VJ5aIaA"
      },
      "source": [
        "# read in the raw data and define the test and training datasets \n",
        "def load_raw_data(path):\n",
        "  train_raw = pd.read_json(path+'train.json', lines=True)\n",
        "  train = train_raw[0:1800]\n",
        "  test = train_raw[1800:2400]\n",
        "  x_train_seq = train['sequence']\n",
        "  x_test_seq = test['sequence']\n",
        "  #y_train = [train['reactivity'], train['deg_Mg_pH10'], train['deg_pH10'], train['deg_Mg_50C'], train['deg_50C']]\n",
        "  #y_test = [test['reactivity'], test['deg_Mg_pH10'], test['deg_pH10'], test['deg_Mg_50C'], test['deg_50C']]\n",
        "  y_train = [train['reactivity'], train['deg_Mg_pH10'], train['deg_pH10']]\n",
        "  y_test = [test['reactivity'], test['deg_Mg_pH10'], test['deg_pH10']]\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "  y_train = y_train.transpose((1,2,0))\n",
        "  y_test = y_test.transpose((1,2,0))\n",
        "  y_train_reacterr = train_raw[0:1800]['reactivity_error']\n",
        "  y_train_reacterr = np.array(y_train_reacterr)\n",
        "\n",
        "  return x_train_seq, x_test_seq, y_train, y_test, y_train_reacterr\n",
        "\n",
        "# convert each sequence feature to a frequency vector\n",
        "def process_data(x_train_seq, x_test_seq):\n",
        "  x_train = []\n",
        "  x_test = []\n",
        "  for seq in x_train_seq:\n",
        "    x_train.append(seq_to_freq(seq))\n",
        "  for seq in x_test_seq:\n",
        "    x_test.append(seq_to_freq(seq))\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  #print(len(seq_to_freq(train['sequence'][0])))\n",
        "  #print(x_train[0])\n",
        "\n",
        "  return x_train, x_test\n",
        "\n",
        "# adjust y_train reactivity values according to corresponding error (aka weighting)\n",
        "# UNIMPLEMENTED\n",
        "def process_data_y(y_train, y_train_reacterr):\n",
        "  return y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7whIzxwMxS5",
        "outputId": "963acd0d-0c1b-40f5-a2bb-913ab28be611"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  x_train_seq, x_test_seq, y_train, y_test, y_train_reacterr = load_raw_data(PATH)\n",
        "  #print(y_train.shape)\n",
        "  #print(x_train.shape)\n",
        "  x_train, x_test = process_data(x_train_seq, x_test_seq)\n",
        "  y_train = process_data_y(y_train, y_train_reacterr)\n",
        "\n",
        "  best_k = find_best_k(x_train, x_test, y_train, y_test)\n",
        "\n",
        "  print(best_k)\n",
        "\n",
        "  # x_test_unlabeled = load_test_data(PATH)\n",
        "  # y_pred = predict(x_test_unlabeled, x_train, y_train, best_k)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1.1834443177124847\n",
            "2 1.029025056694351\n",
            "3 0.9523750740692599\n",
            "8 0.8814729037884628\n",
            "10 0.8715021135791471\n",
            "13 0.8610573999199985\n",
            "15 0.8562489792092149\n",
            "20 0.8490626303001685\n",
            "25 0.8486649184222247\n",
            "27 0.8487833763354912\n",
            "28 0.8489864492368765\n",
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgrg3oNia4i0"
      },
      "source": [
        "# print(labeled_x_train)\n",
        "# print(len(small_x_train))\n",
        "# print(small_x_train[99])\n",
        "# print(len(small_y_train))\n",
        "# for molec in small_y_train:\n",
        "#   for pos in molec:\n",
        "#     if(pos>100):\n",
        "#       print(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc2BIgJjuEeH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}